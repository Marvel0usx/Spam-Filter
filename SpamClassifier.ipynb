{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to E:...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the set of stopwords from nltk\n",
    "nltk.download(\"stopwords\", \"E:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAM_FOLDER = r\"./spam/\"\n",
    "HAM_FOLDER = r\"./ham/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = []\n",
    "ham = []\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "bag_of_words = []\n",
    "\n",
    "def tokenize(text, category=None):\n",
    "    \"\"\"Function that takes in a line of text and returns tokenized representation.\"\"\"\n",
    "    words = re.findall(\"[a-zA-Z']+\", text)\n",
    "    words = [w.lower() for w in words]\n",
    "    token = []\n",
    "    for word in words:\n",
    "        if len(word) <= 2:\n",
    "            continue\n",
    "        if word not in stopwords_list:\n",
    "            if word not in bag_of_words:\n",
    "                bag_of_words.append(word)\n",
    "            token.append(word)\n",
    "    if category:\n",
    "        category.append(token)\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "def read_files():\n",
    "    for is_spam in (True, False):\n",
    "        parent_path = [HAM_FOLDER, SPAM_FOLDER][is_spam]\n",
    "        category = [ham, spam][is_spam]\n",
    "        for file_path in os.listdir(parent_path):\n",
    "            with open(parent_path + file_path) as f:\n",
    "                text = \"\".join(f.readlines()[2:])\n",
    "                tokenize(text, category)\n",
    "\n",
    "read_files()\n",
    "bag_of_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize training data\n",
    "from collections import Counter\n",
    "\n",
    "n_document = len(spam) + len(ham)\n",
    "n_feature = len(bag_of_words)\n",
    "n_spam = len(spam)\n",
    "n_ham = len(ham)\n",
    "\n",
    "y = np.array([0 for _ in range(n_ham)] + [1 for _ in range(n_spam)])\n",
    "X = np.zeros((n_document, n_feature))\n",
    "\n",
    "for i in range(n_document):\n",
    "    if i < n_ham:\n",
    "        count = Counter(ham[i])\n",
    "    else:\n",
    "        count = Counter(spam[i - n_ham])\n",
    "    for j in range(n_feature):\n",
    "        this_word = bag_of_words[j]\n",
    "        X[i, j] = count[this_word] if this_word in count else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape == (n_document, n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] == y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    @classmethod\n",
    "    def train_test_split(X, y, ratio_of_train=0.8):\n",
    "        df = np.append(X, y.reshape((X.shape[0], 1)), axis=1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # concat y to X so that we can reference by class.\n",
    "        X = np.append(X, y.reshape((X.shape[0], 1)), axis=1)\n",
    "        \n",
    "        # our priori is the proportion of spam.\n",
    "        self.priori = n_spam / n_document\n",
    "        \n",
    "        # then, we need to calculate P(word | class) for each word.\n",
    "        # here, we select the words only\n",
    "        words_given_spam = X[X[:,-1] == 1,:-1]\n",
    "        words_given_ham = X[X[:,-1] == 0,:-1]\n",
    "        \n",
    "        # calculate P(word | class) = count(word in class) / count(words in class) plus Laplace smoothing.\n",
    "        self.p_words_given_spam = (np.sum(words_given_spam, axis=0) + 1) / (np.sum(words_given_spam) + n_feature)\n",
    "        self.p_words_given_ham = (np.sum(words_given_ham, axis=0) + 1) / (np.sum(words_given_ham) + n_feature)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        # calculate for the likelihood of X given the class of spam/ham.\n",
    "        words = [w for w in re.findall(\"[a-zA-Z']+\", text.lower()) if w not in stopwords_list]\n",
    "        count = Counter(words)\n",
    "        X = np.zeros(n_feature)\n",
    "        for i in range(n_feature):\n",
    "            if bag_of_words[i] in count:\n",
    "                X[i] = count[bag_of_words[i]]\n",
    "                \n",
    "        # here, we multiply P(x_i | c) together\n",
    "        p_words_given_spam = np.product(np.power(self.p_words_given_spam.astype(\"float64\"), X)) * self.priori\n",
    "        p_words_given_ham = np.product(np.power(self.p_words_given_ham.astype(\"float64\"), X)) * (1 - self.priori)\n",
    "        \n",
    "        if p_words_given_spam > p_words_given_ham:\n",
    "            return \"spam\"\n",
    "        else:\n",
    "            return \"ham\"\n",
    "    \n",
    "    def confusion_matrix(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MultinomialNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_HAM_PATH = \"./test_ham/\"\n",
    "TEST_SPAM_PATH = \"./test_spam/\"\n",
    "\n",
    "test_spam_res = []\n",
    "test_ham_res = []\n",
    "\n",
    "for path in os.listdir(TEST_SPAM_PATH):\n",
    "    file_path = TEST_SPAM_PATH + path\n",
    "    with open(file_path, \"r\") as f:\n",
    "        test_spam_res.append(nn.predict(\"\".join(f.readlines()[2:])))\n",
    "\n",
    "for path in os.listdir(TEST_HAM_PATH):\n",
    "    file_path = TEST_HAM_PATH + path\n",
    "    with open(file_path, \"r\") as f:\n",
    "        test_ham_res.append(nn.predict(\"\".join(f.readlines()[2:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_spam = Counter(test_spam_res)\n",
    "count_ham = Counter(test_ham_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'spam': 45, 'ham': 36})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 1995, 'spam': 17})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNR(0.99)    FPR(0.44)\n",
      "FNR(0.01)    TPR(0.56)\n"
     ]
    }
   ],
   "source": [
    "# The score of our naive-bayes\n",
    "fpr, tpr, tnr, fnr = 36 / (45 + 36), 45 / (45 + 36), 1995 / (1995 + 17), 17 / (1995 + 17)\n",
    "print(f\"TNR({tnr:.2f})    FPR({fpr:.2f})\\nFNR({fnr:.2f})    TPR({tpr:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is using model provided by sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a nn model with Laplace smoothing, learning class priori\n",
    "nn = MultinomialNB(alpha=1, fit_prior=True)\n",
    "nn = nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99625"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score of the training set\n",
    "nn.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare testing set\n",
    "testing_words = []\n",
    "testing_labels = []\n",
    "\n",
    "for path in os.listdir(TEST_SPAM_PATH):\n",
    "    file_path = TEST_SPAM_PATH + path\n",
    "    with open(file_path, \"r\") as f:\n",
    "        testing_words.append([w for w in re.findall(\"[a-zA-Z']+\", \"\".join(f.readlines()[2:]).lower()) if w not in stopwords_list])\n",
    "        testing_labels.append(1)\n",
    "\n",
    "for path in os.listdir(TEST_HAM_PATH):\n",
    "    file_path = TEST_HAM_PATH + path\n",
    "    with open(file_path, \"r\") as f:\n",
    "        testing_words.append([w for w in re.findall(\"[a-zA-Z']+\", \"\".join(f.readlines()[2:]).lower()) if w not in stopwords_list])\n",
    "        testing_labels.append(0)\n",
    "\n",
    "testing_X = np.zeros((len(testing_words), n_feature))\n",
    "testing_y = np.array(testing_labels)\n",
    "\n",
    "for i in range(len(testing_words)):\n",
    "    count = Counter(testing_words[i])\n",
    "    for j in range(n_feature):\n",
    "        if bag_of_words[j] in count:\n",
    "            testing_X[i,j] = count[bag_of_words[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1990,   22],\n",
       "       [   2,   79]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some statistics of the model\n",
    "testing_y_hat = nn.predict(testing_X)\n",
    "confusion_matrix(testing_y, testing_y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
